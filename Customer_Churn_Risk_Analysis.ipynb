# ================================================
# Mini Project: Customer Churn Risk Analysis
# ================================================

# Goal: Predict which SaaS customers are likely to churn and generate actionable insights.
# Dataset: Telco Customer Churn (can be downloaded from Kaggle)
# Tools: Python, pandas, scikit-learn, matplotlib, seaborn
# ================================================

# Step 0: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report

sns.set(style='whitegrid')
%matplotlib inline

# ================================================
# Step 1: Load the Dataset
# ================================================
data = pd.read_csv('telco_churn.csv')  # replace with your dataset path
print("Dataset Shape:", data.shape)
print(data.head())
print("\nChurn Distribution:")
print(data['Churn'].value_counts())

# ================================================
# Step 2: Data Cleaning & Preprocessing
# ================================================
# Convert TotalCharges to numeric
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')

# Drop rows with missing values
data = data.dropna()

# Encode categorical features
categorical_cols = data.select_dtypes(include='object').columns.tolist()
categorical_cols.remove('customerID')  # ID not a feature
categorical_cols.remove('Churn')       # label

# One-hot encode categorical variables
data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)

# Encode target
data_encoded['Churn'] = data_encoded['Churn'].map({'Yes':1, 'No':0})

# Features and target
X = data_encoded.drop(['customerID', 'Churn'], axis=1)
y = data_encoded['Churn']

print("\nFeatures Shape:", X.shape)
print("Target Shape:", y.shape)

# ================================================
# Step 3: Train/Test Split
# ================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# ================================================
# Step 4: Feature Scaling (Important for Logistic Regression)
# ================================================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ================================================
# Step 5: Baseline Models
# ================================================

# --- Logistic Regression ---
lr = LogisticRegression(max_iter=5000)  # increased max_iter to ensure convergence
lr.fit(X_train_scaled, y_train)

y_pred_lr = lr.predict(X_test_scaled)
y_prob_lr = lr.predict_proba(X_test_scaled)[:,1]

print("\n--- Logistic Regression ---")
print("ROC-AUC:", round(roc_auc_score(y_test, y_prob_lr), 3))
print(classification_report(y_test, y_pred_lr))

# --- Random Forest ---
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:,1]

print("\n--- Random Forest ---")
print("ROC-AUC:", round(roc_auc_score(y_test, y_prob_rf), 3))
print(classification_report(y_test, y_pred_rf))

# ================================================
# Step 6: Feature Importance (Random Forest)
# ================================================
importances = rf.feature_importances_
features = X.columns

feat_imp = pd.DataFrame({'feature': features, 'importance': importances})
feat_imp = feat_imp.sort_values(by='importance', ascending=False).head(10)

# Plot top 10 features
plt.figure(figsize=(10,6))
sns.barplot(x='importance', y='feature', data=feat_imp)
plt.title('Top 10 Feature Importances')
plt.show()

# ================================================
# Step 7: Exploratory Insights
# ================================================

# Tenure vs Churn
plt.figure(figsize=(6,4))
sns.boxplot(x='Churn', y='tenure', data=data)
plt.title('Tenure vs Churn')
plt.show()

# MonthlyCharges vs Churn
plt.figure(figsize=(6,4))
sns.boxplot(x='Churn', y='MonthlyCharges', data=data)
plt.title('Monthly Charges vs Churn')
plt.show()

# ================================================
# Step 8: Identify High-Risk Users
# ================================================
data['churn_prob'] = rf.predict_proba(X)[:,1]

# Top 10 high-risk users
high_risk_users = data[['customerID', 'churn_prob']].sort_values(by='churn_prob', ascending=False).head(10)
print("\nTop 10 High-Risk Customers:")
print(high_risk_users)

# ================================================
# Step 9: Summary & Business Takeaways
# ================================================
print("""
Key Insights:
1. Early engagement (tenure, usage) is the strongest predictor of churn.
2. Higher monthly charges and plan type impact retention risk.
3. Simple interventions (nudges, onboarding emails) can reduce churn for high-risk users identified above.
4. Random Forest provides interpretable feature importance for actionable decisions.
""")
